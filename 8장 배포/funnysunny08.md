## 8.1 논리적에서 물리적으로

### 8.1.1 다수 인스턴스

<img width="729" height="248" alt="Image" src="https://github.com/user-attachments/assets/9637352e-3656-444a-b208-36cc73e5cd43" />

- 대부분의 경우 각 마이크로서비스는 하나 이상의 인스턴스로 실행된다.
    - 다수의 인스턴스를 사용하면 더 많은 부하를 처리할 수 있고, 특정 인스턴스가 장애를 일으키더라도 서비스 전체가 중단되지 않으므로 시스템의 가용성이 향상된다.
- 적절한 인스턴스 수는 애플리케이션의 특성에 따라 달라진다.
    - 예상 트래픽 규모, 허용 가능한 장애 수준, 비용 제약 등을 종합적으로 고려해 결정해야 한다.
- 더 높은 가용성이 요구되는 경우, 단일 데이터 센터나 가용 영역에만 의존하지 않고 여러 데이터 센터에 인스턴스를 분산 배치해야 할 수도 있다.
    - AWS, Azure, Google Cloud와 같은 주요 클라우드 제공자들은 단일 머신이나 단일 가용 영역 수준의 SLA를 제공하지 않는다.
    - 따라서 진정한 고가용성을 원한다면 물리적으로 분리된 인프라에 인스턴스를 배치해야 한다.

### 8.1.2 데이터베이스

- 논리적으로 동일한 마이크로서비스의 여러 인스턴스는 어느 정도의 상태를 공유해야 한다.
    
    <img width="822" height="673" alt="Image" src="https://github.com/user-attachments/assets/cf0287f9-ad86-4d2b-8b73-a1616217b677" />
    
- 여기서 흔히 제기되는 질문은 이것이 “데이터베이스를 공유하지 말라”는 마이크로서비스 원칙을 위반하는가이다.
    - 결론부터 말하면 위반이 아니다.
    - 문제되는 것은 여러 서로 다른 마이크로서비스가 하나의 데이터베이스를 공유하면서, 데이터 접근 로직과 변경 로직이 여러 서비스에 분산되는 경우다.
- 이 경우 데이터는 동일한 마이크로서비스의 서로 다른 인스턴스 간에만 공유된다.
    - 데이터에 접근하고 조작하는 로직은 여전히 논리적으로 하나의 마이크로서비스 내부에 존재한다.
    - 따라서 서비스 경계는 유지되며, 마이크로서비스 아키텍처의 핵심 원칙도 훼손되지 않는다.

**📍 데이터 배포 및 확장**

<img width="845" height="707" alt="Image" src="https://github.com/user-attachments/assets/19f5b6de-ac74-45d6-8c91-ded4a01a48a5" />

- 물리적인 데이터베이스는 다양한 이유로 여러 머신에 분산 배포된다.
    - 대표적인 예가 Primary–Replica 구조로, 쓰기 작업은 Primary 노드에서 처리하고 읽기 작업은 하나 이상의 읽기 전용 노드로 분산해 처리한다.
- 이 구조에 더해, 하나의 데이터베이스 인프라스트럭처가 논리적으로 분리된 여러 데이터베이스를 동시에 제공할 수도 있다.
    - 예를 들어 송장 데이터베이스와 주문 데이터베이스가 동일한 데이터베이스 엔진과 하드웨어 위에서 실행될 수 있다.
        
        <img width="1038" height="535" alt="Image" src="https://github.com/user-attachments/assets/2a6ed8cf-69f8-4730-ad0e-494a8bc77da0" />
        
- 이렇게 하면 하드웨어 자원을 효율적으로 풀링할 수 있고 라이선스 비용을 절감할 수 있으며 데이터베이스 운영 및 관리 작업을 줄일 수 있다.
- 다만 중요한 점은 동일한 하드웨어와 엔진을 사용하더라도 이 데이터베이스들은 논리적으로 완전히 격리되어 있어야 하며 서로 간섭해서는 안 된다는 것이다.
- 또한 공유 데이터베이스 인프라스트럭처 자체가 장애를 일으킬 경우 여러 마이크로서비스가 동시에 영향을 받을 수 있다는 점은 반드시 고려해야 한다.
- 온프레미스 환경을 직접 운영하는 조직은 비용 절감 및 운영 효율성 때문에 여러 데이터베이스를 하나의 공유 인프라에서 호스팅할 가능성이 높다.
- 반면, 공용 클라우드 환경에서는 마이크로서비스마다 전용 데이터베이스 인프라를 프로비저닝하는 경우가 훨씬 많다.
    - 이는 비용이 상대적으로 저렴하고 자동화가 잘 되어 있기 때문이다.
    - 예를 들어 AWS RDS는 백업, 업그레이드, 다중 가용 영역 페일오버 등을 기본 기능으로 제공하며, 다른 클라우드 공급자도 유사한 관리형 서비스를 제공한다.

### 8.1.3 환경

<img width="997" height="587" alt="Image" src="https://github.com/user-attachments/assets/cb98f90f-2206-498a-8893-9ff34b5183ef" />

- 각 환경은 소프트웨어를 개발하고, 테스트하고, 운영에 적합한지 검증하기 위해 **서로 다른 목적**을 가진다.
    - 하나의 마이크로서비스는 최종 사용자가 접근하는 운영 환경에 배포되기 전까지 개발 환경, 테스트 환경, 스테이징 환경 등 여러 환경을 순차적으로 거친다.
- 이상적으로는 이 모든 환경이 **운영 환경의 정확한 복사본**이면 가장 좋다.
    - 운영 환경에서의 동작을 사전에 최대한 정확히 검증할 수 있기 때문
- 하지만 현실적으로는 다음과 같은 제약이 존재한다.
    - 비용 문제
    - 인프라 구축 및 유지 관리 부담
    - 빠른 피드백에 대한 요구
- 특히 개발자에게는 가능한 한 빨리 소프트웨어가 제대로 동작하는지를 확인하고 문제를 신속히 수정할 수 있는 환경이 중요하다.
- 따라서 일반적으로는 절충안이 선택된다. → 9장

<img width="793" height="532" alt="Image" src="https://github.com/user-attachments/assets/4ea7d282-e34d-4e2e-8927-0debc988d22b" />

- 중요한 점은 환경마다 마이크로서비스의 물리적 토폴리지가 달라질 수 있다는 것이다.
- 환경에 따라 인스턴스 수가 달라질 수 있고 사용되는 인프라 구성도 달라질 수 있다.
- 따라서 환경별 구성을 명확히 분리하고 한 환경에서 다른 환경으로 이동할 때 인스턴스 수를 유연하게 변경할 수 있어야 한다.
- 또한 서비스 인스턴스는 **한 번만 빌드**하고 환경별 설정 정보는 빌드 산출물과 분리해 관리하는 것이 바람직하다.

## 8.2 마이크로서비스 배포의 원칙

### 8.2.1 격리 실행

- 여러 마이크로서비스 인스턴스를 하나의 머신에 몰아넣으면 관리와 비용 측면에서는 단순해 보이지만 문제가 많다.
    - 리소스 경합으로 인해 특정 서비스의 부하가 다른 서비스에 영향을 준다.
    - 모니터링과 장애 추적이 어렵다.
    - 한 서비스의 배포가 다른 서비스에 영향을 미칠 수 있다.
    - 배포 시 조율이 필요해져 팀 자율성과 독립 배포가 약화된다.
- 따라서 마이크로서비스 인스턴스는 격리된 실행 환경에서 실행되어야 한다.
- 컨테이너와 VM은 대부분의 워크로드에서 충분한 격리를 제공하며, 특히 컨테이너는 비용 효율성과 빠른 프로비저닝 덕분에 널리 사용된다.
    - 물리 머신: 가장 강력한 격리가 가능하지만 가장 높은 비용이 발생한다.
- AWS Lambda, Heroku 같은 추상화된 플랫폼도 내부적으로 격리된 실행 환경을 제공한다.

### 8.2.2 자동화 집중

- 마이크로서비스의 수가 늘어날 수록 운영해야 할 프로세스, 관리해야 할 configuration, 모니터링해야 할 인스턴스 등이 함께 증가한다.
    - 따라서 마이크로서비스 환경에서는 **자동화에 지속적으로 집중해야 한다.**
        - **→** 개발자의 생산성을 유지하고, 개발자가 스스로 서비스를 프로비저닝하고 배포할 수 있도록 돕는 핵심 요소
- 자동화를 가능하게 하는 기술을 선택할 때는 다음과 같은 질문에 답할 수 있어야 한다.
    - 가상 머신이나 컨테이너를 코드로 시작하고 종료할 수 있는가?
    - 소프트웨어 배포를 완전히 자동화할 수 있는가?
    - 데이터베이스 변경 사항을 수작업 없이 배포할 수 있는가?

### 8.2.3 코드형 인프라스트럭처

- 코드형 인프라스트럭처(Infrastructure as Code)는 기계가 읽을 수 있는 코드로 인프라스트럭처를 정의하고 관리하는 개념이다.
- 셰프(Chef), 퍼핏(Puppet), 안시블(Ansible) 같은 도구를 사용하거나 설정용 스크립트를 통해 시스템을 “알려진 상태”로 전환할 수 있다.
- 이 개념의 핵심 가치는 다음과 같다.
    - 인프라 구성을 코드로 정의함으로써 버전 관리가 가능해지고 변경 이력을 추적할 수 있으며 테스트와 반복 실행이 가능해진다.
    - 즉, 소프트웨어 개발의 방식을 운영 영역으로 확장한 것이다.
- 이 분야에는 다양한 도구가 존재한다.
    - 전통적인 구성 관리 도구: CFEngine, Puppet, Chef, Ansible
    - 클라우드 리소스 전반을 관리하는 도구: Terraform
    - 일반 프로그래밍 언어 기반 접근: Pulumi
    - 특정 플랫폼 도구: AWS CloudFormation, AWS CDK
    - 필자는 AWS 환경에서도 테라폼과 같은 크로스 플랫폼 도구의 유연성을 선호한다고 밝힌다.
- 코드형 인프라스트럭처의 또 다른 장점은 감사와 재현성이다.
    - 누가 언제 무엇을 변경했는지 명확히 알 수 있고, 과거 특정 시점의 환경을 다시 재현할 수 있다.

### 8.2.4 무중단 배포

- 무중단 배포를 구현하지 못한다면, 릴리즈 시마다 업스트림 소비자에게 잠재적 중단을 사전 공지하고 조율해야 할 수도 있다.
- 무중단 배포의 가장 큰 이점은 배포 속도의 향상이다.
    - 무중단 배포가 가능해지자 릴리즈 빈도가 크게 증가했고 업무 시간 중 배포가 가능해졌다.
- 무중단 배포의 목표는 업스트림 소비자가 배포 사실을 인지하지 못하게 하는 것이다.
    - 비동기 메시징 기반 통신을 사용하는 경우 상대적으로 구현이 쉽다.
    - 동기식 통신을 사용하는 경우 더 많은 고려가 필요하다.
- 롤링 업그레이드는 대표적인 기법이다.
    - 새 버전 인스턴스를 점진적으로 늘리고 기존 인스턴스를 서서히 종료한다.
    - 쿠버네티스 같은 플랫폼은 이를 쉽게 지원한다.
- 다만, 무중단 배포만을 목적으로 복잡한 플랫폼을 도입하는 것은 과도할 수 있다.
    - 8.6.1절에서 다룰 블루–그린 배포만으로도 충분한 경우가 많다.
- 무중단 배포를 염두에 두고 서비스를 설계하는 것이 나중에 이를 억지로 덧붙이는 것보다 훨씬 수월하다.

### 8.2.5 기대 상태 관리

<img width="1181" height="628" alt="Image" src="https://github.com/user-attachments/assets/687689b9-35d1-425d-ae8d-b52e23f9264a" />

- 기대 상태 관리(Desired State Management)는 시스템이 항상 “원하는 상태”를 유지하도록 플랫폼이 자동으로 관리하는 방식이다.
    - 실행 중인 시스템이 기대 상태에서 벗어나면 플랫폼이 자동으로 이를 감지하고 다시 기대 상태로 되돌린다.
- 예를 들어, 특정 마이크로서비스의 인스턴스 수, 필요한 CPU, 메모리 자원을 정의해두면 플랫폼은 이를 충족하도록 시스템을 조정한다.
    - 인스턴스 하나가 죽으면 플랫폼은 현재 상태가 기대 상태와 다르다는 것을 인식하고 대체 인스턴스를 자동으로 시작한다.
- 장점
    - 운영자가 “어떻게”를 고민할 필요가 없다.
    - “무엇이 되어야 하는지”만 정의하면 된다.
    - 하드웨어 장애, 인스턴스 종료, 데이터 센터 중단에도 사람의 개입 없이 시스템이 복구될 수 있다.
- 대표적인 플랫폼 예시 → 쿠버네티스, AWS Auto Scaling Group, Azure의 유사 기능, 노마드(컨테이너 외에도 VM, 배치 작업 등 다양한 워크로드를 유연하게 지원)

**📍전제 조건**

- 기대 상태 관리를 위해서는 마이크로서비스 인스턴스를 자동으로 시작할 수 있어야 하며 배포 과정이 완전히 자동화되어 있어야 한다.
- 기동이 느리다면 장애 시를 대비해 여분의 용량을 확보해야 한다.
- 플랫폼을 직접 구현하기보다는 이 개념을 기본 철학으로 채택한 플랫폼을 사용하는 것이 일반적으로 더 낫다.

**📍깃옵스(GitOps)**

- 깃옵스는 코드형 인프라스트럭처와 기대 상태 관리 개념을 결합한 접근 방식이다.
- 원하는 시스템 상태를 코드로 정의하고 이를 Git 저장소에 저장한다.
- 변경 사항이 발생하면 도구가 이를 실행 중인 시스템에 반영한다.

## 8.3 배포 방법

### 8.3.1 물리 머신

- 정의: 마이크로서비스를 가상화 또는 컨테이너화 계층 없이 물리 머신에 직접 배포하는 방식
- 쇠퇴 이유:
    - 낮은 자원 활용도: 하나의 마이크로서비스 인스턴스가 실행될 때 물리 머신의 CPU, 메모리, I/O 등 자원을 절반만 사용하면 나머지 자원은 낭비된다.
    - 격리 환경 침해: 동일한 물리 머신에 여러 마이크로서비스를 함께 배포할 경우 서비스 간 격리 실행 환경 원칙을 위반하게 된다.
    - 관리의 어려움: 단일 물리 머신 수준에서 기대 상태 관리, 무중단 배포 등을 구현하려면 퍼핏, 셰프와 같은 도구 외에 추가적인 관리 계층이 필요하며, 이는 일반적으로 가상 머신과 함께 사용될 때 더 효과적이다.
- 결론: 현재는 거의 사용되지 않으며, 물리 머신 배포를 정당화할 매우 구체적인 요구 사항이나 제약 조건이 있어야만 고려되는 방식이다. 가상화나 컨테이너화가 제공하는 유연성이 훨씬 뛰어나다.

### 8.3.2 가상 머신

- 영향: 기존 물리 머신을 더 작은 가상 머신으로 분할
    - 인프라스트럭처 사용도를 높이고 호스트 관리 부담을 줄이는 장점이 있다.
- 작동 방식:
    - 하나의 물리 머신을 여러 '가상' 머신으로 분할한다.
    - 각 가상 머신은 소프트웨어에 일반 서버처럼 동작하며, CPU, 메모리, I/O, 스토리지가 할당된다.
    - 마이크로서비스 인스턴스를 위해 격리된 실행 환경을 제공한다.
    - 모든 가상 머신은 완전한 운영체제와 자체 리소스 세트를 포함한다.
    - 매우 훌륭한 인스턴스 간 격리 수준을 보장한다.
    - 각 마이크로서비스 인스턴스는 VM 내의 운영체제를 독립적으로 구성할 수 있다.
- 문제점: 하부 하드웨어 고장 시 여러 마이크로서비스 인스턴스가 손실될 수 있어 기대 상태 관리 등의 해결책이 필요하다.

**📍가상화 비용**

<img width="660" height="367" alt="Image" src="https://github.com/user-attachments/assets/9b0635fa-cded-49c5-be52-876224b8da08" />

- 동일한 하드웨어에 더 많은 VM을 넣을수록 컴퓨팅 리소스는 늘어나지만, VM 자체의 효용은 줄어드는 현상 발생
    - 이는 가상화가 사용하는 오버헤드 때문 (양말 서랍 칸막이 비유)
- 오버헤드의 원인 (타입 2 가상화 기준):
    - 물리적 인프라에 호스트 운영체제가 존재한다.
    - 호스트 OS 위에서 하이퍼바이저가 실행되며 두 가지 주요 임무를 수행한다.
        1. CPU, 메모리 등 물리 자원을 가상 호스트에 매핑한다.
        2. 제어 계층 역할을 하여 가상 머신을 조작할 수 있게 한다.
    - VM 내부는 별도의 호스트처럼 보이며, 자체 커널로 운영체제를 실행한다.
    - 하이퍼바이저는 작업을 위해 별도의 자원(CPU, I/O, 메모리)을 확보해야 하므로, VM이 사용할 수 있는 자원이 줄어든다.
    - 하이퍼바이저가 관리하는 호스트가 많을수록 더 많은 자원이 필요하며, 특정 시점에서는 이 오버헤드가 물리 인프라의 추가 분할을 제약한다.

**📍마이크로서비스에 적합한가?**

- 장점:
    - 격리 측면에서 매우 훌륭하다.
    - 구글 클라우드, 애저, AWS 같은 관리형 VM은 자동화 API와 도구 생태계를 잘 지원하며, 자동 확장 그룹 등으로 기대 상태 관리를 돕는다.
- 단점:
    - 가상화 오버헤드로 인한 비용이 발생한다.
    - (VMware 같은) 전통적 가상화 플랫폼은 중앙 통제형으로 자동화 기능이 제한적일 수 있다.
- 활용 사례: 넷플릭스는 AWS EC2를 통해 많은 마이크로서비스를 성공적으로 구축했다.
- 선택 기준: 더 엄격한 격리 수준이 필요하거나 애플리케이션을 컨테이너화하기 어려운 경우 VM이 좋은 선택이 될 수 있다.

### **8.3.3 컨테이너**

컨테이너는 서버 측 소프트웨어 배포의 지배적인 개념이 되었으며, 마이크로서비스 아키텍처를 패키징하고 실행하기 위한 실질적인 선택지로 자리 잡았다. 도커에 의해 대중화되었고 쿠버네티스와 같은 컨테이너 오케스트레이션 플랫폼과 결합되어 대규모 마이크로서비스 아키텍처 실행의 주요 선택지가 되었다.

**📍다른 방식의 격리**

- 작동 원리:
    - 컨테이너는 가상 머신과 달리 동일한 하부 커널을 공유한다.
    - 리눅스 커널이 프로세스와 자원을 관리하는 방식을 활용하여, 컨테이너는 전체 시스템 프로세스 트리의 하위 추상화 형태로 동작한다. 커널이 모든 자원 할당을 처리한다.
    - 이러한 접근 방식은 LXC(Linux Containers)를 통해 주류화되었고, 도커는 컨테이너에 대한 더 높은 수준의 추상화를 제공하며 보편화시켰다.
- 가상 머신과의 차이점:
    - 컨테이너는 하이퍼바이저가 필요 없다.
    - 자체 커널이 없어 호스트 머신의 커널을 재사용하며, 이로 인해 자원 사용 효율이 높다.
- 장점:
    - 하이퍼바이저 오버헤드 없이 자원 효율성이 뛰어나다.
    - 가상 머신보다 훨씬 빠르게 프로비저닝 및 시작될 수 있다. (몇 초 단위)
    - 리소스 할당을 정밀하게 제어하여 하드웨어를 최대한 활용한다.
    - 가상 머신보다 더 많은 컨테이너를 동일한 하드웨어에서 실행할 수 있어 비용 효율적이다.
- 컨테이너는 완전한 가상화 환경(예: AWS EC2 인스턴스)에서도 사용될 수 있다.

**📍완전하지는 않다**

- 관리의 복잡성: 컨테이너 수가 많아질수록 외부 트래픽 라우팅 및 전체적인 관리가 복잡해질 수 있으며, 도커 같은 도구가 이를 돕는다.
- 격리 수준: 컨테이너의 리소스 격리(CPU, 메모리 제한 등)는 가상 머신만큼 완벽하지 않을 수 있다.
    - 초기에는 컨테이너 간 또는 호스트와의 상호작용 취약점이 존재했지만, 컨테이너 오케스트레이션 시스템의 발전으로 격리 수준이 크게 개선되었다.
    - 일반적으로 신뢰할 수 있는 소프트웨어를 실행할 때는 격리에 문제가 없으나, 악의적인 사용자에 대한 강력한 격리가 필요할 경우 추가적인 고려가 필요하다.

> **📍모호한 경계**
> 
> - 컨테이너의 경량성을 유지하면서 가상 머신의 강력한 격리 수준을 제공하려는 노력이 활발하다.
> - 예시: 마이크로소프트의 Hyper-V 컨테이너, 그리고 AWS 람다(Lambda) 및 파게이트(Fargate) 서비스에 사용되는 파이어크래커(Firecracker)는 시작 시간을 단축하고 운영 효율을 높이면서도 강력한 격리를 제공하는 커널 기반 VM 기술이다.

**📍도커 (Docker)**

- 주요 기능: 도커 툴체인은 컨테이너 프로비저닝, 네트워킹 문제 처리, 도커 애플리케이션을 저장하는 레지스트리 제공 등 컨테이너 관련 작업 대부분을 관리한다.
- 이미지 개념 도입: 도커는 '이미지' 개념을 도입하여 컨테이너 사용을 훨씬 용이하게 만들었다.
    - 도커 이미지는 마이크로서비스 구현의 세부 사항(예: Go, Python, Node.js 등)을 추상화하여 숨길 수 있다.
    - 이를 통해 모든 마이크로서비스 인스턴스를 관리하는 공통된 도구 집합을 사용할 수 있다.
- 개발 및 테스트 환경 개선:
    - 과거에는 개발용 머신에서 여러 VM을 호스팅하기 위해 Vagrant 같은 무거운 도구를 사용했으나, 도커는 개발자 컴퓨터에서 도커 데스크톱을 통해 쉽게 컨테이너를 실행할 수 있게 한다.
    - 운영 환경에서 실행될 컨테이너 이미지와 동일한 이미지를 로컬에서 빌드하거나 가져와 실행하여 개발 환경과 운영 환경의 일관성을 높인다.
- 단일 머신 관리의 한계:
    - 초기 도커는 단일 머신에서 컨테이너 관리에 중점을 두었다.
    - 여러 머신에서 컨테이너를 관리하기 위한 시도로 '도커 스웜(Docker Swarm)'과 '도커 스웜 모드(Docker Swarm Mode)'가 출시되었다.
    - 하지만 실제로 대규모 컨테이너 관리에는 쿠버네티스(Kubernetes)가 더 우수한 솔루션으로 평가된다.

**📍마이크로서비스에 대한 적합성**

- 컨테이너는 마이크로서비스의 특성(격리성, 독립성)과 매우 잘 맞는다.
- 도커의 역할: 도커는 컨테이너를 실용적인 기술로 만들어 마이크로서비스에 적용 가능하게 했다.
    - 감당할 수 있는 비용으로 강력한 격리성을 제공한다.
    - 하부 기술 스택을 추상화하여 다양한 기술 스택의 마이크로서비스를 혼합 운영할 수 있게 한다.
- 한계: 기대 상태 관리(Desired State Management)와 같은 고급 개념을 구현하려면 쿠버네티스와 같은 오케스트레이션 도구가 필요하다.

### **8.3.4 애플리케이션 컨테이너**

- IIS, WebLogic, Tomcat과 같은 단일 애플리케이션 컨테이너에 여러 서비스나 애플리케이션을 배포하는 방식이다.
    - 클러스터링 지원, 모니터링 도구 등 관리 용이성을 제공하며, 언어 런타임 오버헤드를 줄이는 이점이 있다. (예: 하나의 JVM에서 여러 Java 서비스 실행)
- 단점:
    - 기술 선택 제한: 특정 기술 스택에 종속되어 기술 선택의 폭과 자동화 및 관리 유연성이 저하된다.
    - 기능적 한계: 인메모리 세션 상태를 저장하는 세션 클러스터링은 확장 시 문제를 일으킬 수 있다. 제공하는 모니터링 기능도 MSA의 통합 모니터링 요구사항을 충족하기 어렵다.
    - 느린 시작 시간: 개발자 피드백 주기에 부정적인 영향을 미친다.
    - 관리 복잡성: JVM과 같은 플랫폼에서 애플리케이션 수명 주기 관리, 리소스 사용 및 스레드 분석이 복잡하다.
    - 리소스 오버헤드: 상용 제품의 경우 비용이 발생하고, 그 자체로 리소스 오버헤드가 추가된다.
    - 부족한 격리 수준: 마이크로서비스 아키텍처에 필요한 충분한 격리 수준을 제공하지 못하여 점점 사용이 줄고 있다.
- 이 방식은 과거 리소스 부족 상황에 대한 최적화 시도였으나, 오늘날 마이크로서비스 배포 모델에서는 그 부족한 격리 수준으로 인해 드물게 사용된다.

### **8.3.5 PaaS (Platform as a Service)**

- 단일 호스트보다 높은 추상화 수준에서 작동하는 플랫폼이다.
    - Java WAR 파일이나 Ruby Gem과 같은 특정 기술의 산출물을 자동으로 프로비저닝하고 실행한다.
    - 시스템 확장 및 축소를 투명하게 처리하려고 시도한다.
- 특징 및 예시:
    - Heroku와 같은 초기 PaaS 솔루션은 개발자 친화적인 인터페이스를 제공하며, 애플리케이션 인스턴스뿐만 아니라 데이터베이스 인스턴스 등도 함께 제공한다.
    - K-PaaS 컨테이너 플랫폼은 독립된 쿠버네티스 환경을 제공하여 개발자가 손쉽게 클러스터를 구축하고 최신 애플리케이션을 구축할 수 있도록 돕는다.
- 장점: 제대로 작동할 경우, 많은 운영 부분을 PaaS가 대신 처리하여 효율적이다.
- 단점 및 한계:
    - 문제 해결의 어려움: 문제가 발생해도 내부 접근 권한이 제한적이어서 해결이 어려울 수 있다.
    - 유연성 부족: PaaS가 '더 똑똑해지려 할수록' 오히려 문제가 발생할 수 있으며, 애플리케이션이 덜 표준화될수록 PaaS와의 호환성 문제가 생길 가능성이 높다.
    - 적합성 의문: 모든 마이크로서비스 모델에 PaaS가 적합한지 확신하기 어렵다.
- 최신 동향: 저자가 기대했던 방식의 성장이 아닌, 퍼블릭 클라우드 공급자의 서버리스(Serverless) 제품(메시지 브로커, 데이터베이스 등 관리형 솔루션)이 PaaS의 역할을 대체하며 빠르게 성장하고 있다. 특히 FaaS(Function as a Service)가 많은 주목을 받고 있다.
- 적합성: 마이크로서비스와 관련된 PaaS 제품은 형태와 크기가 다양하므로, 애플리케이션 특성에 따라 Heroku, Netlify 등 다양한 플랫폼이 배포에 사용될 수 있다.

### **8.3.6 FaaS (Function as a Service)**

- AWS 람다(Lambda)
- 기본 작동 방식:
    - 코드(함수) 배포: 개발자는 실행할 코드 조각(함수)을 배포한다.
    - 트리거 기반 실행: 코드는 특정 이벤트(트리거)가 발생할 때까지 대기 상태(휴면)로 있다. 트리거는 파일 도착, 메시지 큐 항목 발생, HTTP 요청 등 다양할 수 있다.
    - 실행 및 종료: 함수가 트리거되면 실행되고, 작업이 완료되면 자동으로 종료된다.
    - 플랫폼의 관리: 하부 플랫폼은 필요에 따라 함수를 시작/종료하고, 동시 실행을 처리하며, 고가용성을 제공한다.
- 주요 이점:
    - 비용 효율성: 실행되지 않는 시간에는 비용이 발생하지 않으며, 사용한 만큼만 지불한다 (Pay-per-use). 트래픽 예측이 어렵거나 부하가 적은 상황에 유리하다.
    - 운영 부담 감소: 플랫폼이 함수 시작/종료, 고가용성 등을 자동 처리하여 운영 관련 걱정을 크게 줄여준다.

**📍제한 사항**

- 런타임 제어 부족
    - 대부분의 FaaS는 내부적으로 컨테이너 기술을 사용하지만, 개발자에게는 숨겨져 있다.
- 리소스 할당 제한
    - 함수에 할당되는 리소스(CPU, I/O)는 메모리 할당량에 따라 결정되는 경향이 있어, 메모리가 필요하지 않아도 CPU를 더 많이 사용하려면 메모리를 늘려야 하는 비효율이 발생할 수 있다.
- 대부분의 FaaS는 함수 실행 시간에 제한이 있다. (예: AWS 람다는 최대 15분, Google Cloud Functions는 9분)
- 무상태(Statelessness):
    - 함수 호출은 기본적으로 무상태로 간주된다. 즉, 이전 호출의 상태를 직접 유지하지 않으며, 상태 관리는 데이터베이스와 같은 외부 저장소에서 이루어져야 한다.
    - 단, 애저 영속 함수(Azure Durable Functions)처럼 이 문제를 해결하여 함수 호출 간 상태 유지를 지원하는 예외도 있다.

**📍문제점**

- 콜드 스타트(Cold Start):
    - 필요할 때마다 함수 인스턴스를 새로 시작하는 데 시간이 걸리는 현상
    - 하지만 최근 FaaS 공급업체들의 내부 최적화로 인해 콜드 스타트의 영향은 크게 줄어들었다.
- 동적 확장 불균형:
    - 함수는 매우 빠르게 동적으로 확장할 수 있지만, 시스템의 다른 부분(예: 데이터베이스, 메시지 큐 등)이 이에 상응하는 확장성을 가지지 못하면 병목 현상이나 과부하를 초래할 수 있다.

**📍마이크로서비스와 배포 방식 매핑**

마이크로서비스 인스턴스는 가상 머신, 컨테이너 등 다양한 배포 메커니즘에 매핑될 수 있지만, FaaS에서는 매핑 전략이 더 중요해집니다.

1. 마이크로서비스당 함수 매핑
    - 단일 마이크로서비스 인스턴스를 하나의 FaaS 함수로 배포하는 모델
    - 특징: 함수에 단일 진입점이 있으며, 인바운드 요청 경로에 따라 마이크로서비스 내부의 다양한 기능으로 분기 처리한다.
    - 장점: 마이크로서비스의 배포 단위 개념을 유지하며 FaaS의 효율성을 활용한다.
2. 애그리거트당 함수 매핑
    - 세분화: 마이크로서비스를 도메인 기반 설계의 애그리거트(논리적 객체 묶음) 단위로 분할하여, 각 애그리거트를 하나의 FaaS 함수로 매핑한다.
    - 장점:
        - 각 애그리거트의 로직이 함수 내에 완전히 포함되어 수명 주기 관리가 용이한다.
        - 마이크로서비스가 물리적 단위보다는 여러 함수로 구성된 논리적 개념에 가까워진다.
    - 고려사항:
        - 외부 인터페이스: 소비자에게는 여전히 단일 마이크로서비스로 보이도록 해야 한다.
        - 데이터 분리: 초기에는 공유 데이터베이스 사용도 가능하나, 시간이 지남에 따라 각 애그리거트 함수의 요구 사항에 따라 데이터 사용을 분리하는 경향이 있다.
3.  과도한 세분화는 지양
    - 애그리거트보다 더 작은 단위로 함수를 분할하는 것은 관리 복잡도를 증가시키고, 애그리거트의 일관성 원칙을 위반할 수 있어 권장되지 않는다. (예: 각 상태 전환을 개별 함수로 만드는 것)

**📍FaaS의 미래 전망**

- FaaS는 하부 인프라의 상세를 숨겨 운영 부담을 줄이고 비용 효율성을 높이는 중요한 플랫폼으로 부상하고 있다.
- 콜드 스타트, 리소스 제어, 실행 시간 제한 등 현재의 제약 사항들이 있지만, 기술 발전으로 점차 완화되고 있다.
- 모든 애플리케이션에 적합한 것은 아니지만, 특정 사용 사례(예: BBC 뉴스 웹사이트)에서는 큰 이점을 제공하며, 쿠버네티스 등 다른 기술과의 결합으로 활용 범위가 넓어지고 있다.
- 결론적으로, FaaS는 상당한 이점을 제공하며 복잡성을 숨겨주므로, 클라우드 기반 솔루션을 고려할 때 우선적으로 탐색해 볼 가치가 있다.

## **8.4 어떤 배포가 적합할까?**

1. 현재 작동하고 있다면 바꾸지 않는다.
2. 점진적으로 통제권을 포기한다.
    - Heroku나 FaaS 플랫폼처럼 훌륭한 PaaS 솔루션이 애플리케이션에 적합하다면, 운영 관리를 플랫폼에 맡기고 제품 개발에 집중한다. 이를 통해 개발 생산성을 크게 높일 수 있다.
    - "쿠버네티스 아니면 망한다"는 식의 맹목적인 추종은 지양한다. 공용 클라우드에서 FaaS가 적합하다면, FaaS를 우선 고려하여 개발자 생산성을 높이는 것이 현명하다.
3. 컨테이너화를 고려한다.
    - 컨테이너는 격리성과 비용 사이의 좋은 절충안을 제공하며, 로컬 개발에도 큰 이점을 준다.
    - PaaS/FaaS가 적합하지 않은 경우에는 컨테이너화가 주요 대안이 된다. 이때, 여러 컨테이너를 효율적으로 관리하기 위해 쿠버네티스를 고려한다.

## 8.5 쿠버네티스와 컨테이너 오케스트레이션

### 8.5.1 컨테이너 오케스트레이션에 대한 사례

- 쿠버네티스 → 컨테이너 오케스트레이션 플랫폼, 컨테이너 스케줄러
- 컨테이너 오케스트레이션 플랫폼은 컨테이너 워크로드가 실행되는 방법과 위치를 다룬다.
- 운영자가 실행을 요청하면 오케스트레이터는 그 작업을 스케줄링하는 방법을 수행한다.
    - 가용한 자원을 찾아 할당하고 운영자를 위해 세세한 일을 처리한다.
- 기대 상태 관리 제공

### 8.5.2 쿠버네티스의 개념 엿보기

<img width="1034" height="737" alt="Image" src="https://github.com/user-attachments/assets/d42c8634-d7ed-4fc3-b48c-947c8bf53e81" />

- 워크로드가 실행될 머신 집합을 `노드` 라고 한다.
- 이 노드를 관리하는 제어 소프트웨어(노드)를 `컨트롤 플레인` 이라고 한다.
    - 이러한 노드는 내부저긍로 물리 머신 또는 가상 머신을 실행할 수 있다.
- 컨테이너를 스케줄링하는 대신에 `파드` 라는 것을 스케줄링한다.
    - 파드 = 함께 배포될 하나 이상의 컨테이너로 구성
    - 일반적으로 파드 안에는 하나의 컨테이너(e.g. 마이크로서 인스턴스)만 있지만, 여러 컨테이너를 함께 배포하는 것이 합리적인 경우도 드물게 있다.

<img width="1173" height="803" alt="Image" src="https://github.com/user-attachments/assets/f2337f58-b249-4c3c-af2b-d3f067415d1d" />

- `서비스` 는 쿠버네티스 맥락에서 안정적인 라우팅 엔드포인트로 간주할 수 있으며, 실행 중인 파드로부터 클러스터 내에서 사용할 수 있는 안정적인 네트워크 인터페이스로 매핑하는 방법이 된다.
    - 호출과 파드를 상호 라우팅
    - 파드를 종료하거나 시작하는 새로운 파드를 처리
    - 서비스를 배포하지 않고 서비스에 매핑되는 파드를 배포
- `레플리카셋` 을 통해 기대 상태 정의
    - 디플로이먼트는 파드와 레플리카셋에 대한 변경 사항을 적용하는 방법
    - 롤링 업그레이드, 롤백, 노드 수 확장 등과 같은 작업 수행 가능
- 따라서 마이크로서비스를 배포하려면 마이크로서비스 인스턴스를 포함할 `파드`를 정의해야 한다. `서비스`를 정의해 쿠버네티스가 마이크로서비스에 액세스하는 방법을 알려준다. `디플로이먼트`를 사용하면 실행 중인 포드에 변경 사항을 적용할 수 있다.

### 8.5.3 멀티테넌시와 페데레이션

> **멀티테넌시(Multi-tenancy)란?**
> 
> 
> 하나의 소프트웨어 인스턴스 또는 클러스터를 여러 독립적인 사용자 그룹(테넌트)이 공유하여 사용하는 아키텍처를 말한다. 각 테넌트는 마치 자신만의 전용 인스턴스를 사용하는 것처럼 느껴지지만, 실제로는 공유된 인프라를 사용하므로 효율성을 높일 수 있다.
> 
- 효율적인 자원 활용을 위해 조직의 모든 워크로드를 하나의 쿠버네티스 클러스터에 모아 관리하려는 경우가 있다.
- 조직 부서마다 자원에 대한 제어 수준이 다를 수 있는데, 이러한 제어는 쿠버네티스에 내장되어 있지 않다.

이러한 문제를 해결하는 두 가지 주요 방법이 있다.

1. 쿠버네티스 위에 구축된 플랫폼 채택:
    - 멀티테넌시 기능을 제공하는 쿠버네티스 기반 플랫폼을 사용하는 방법
    - e.g. 레드햇의 오픈시프트(OpenShift)
2. 페데레이션(Federation) 모델 고려:
    
    <img width="1216" height="860" alt="Image" src="https://github.com/user-attachments/assets/ce568a8b-9cd9-4bee-8553-9f10667d2703" />
    
    - 여러 개의 분리된 쿠버네티스 클러스터를 가지며, 그 위에 소프트웨어 계층을 두어 필요할 때 모든 클러스터에 변경 사항을 적용하는 방식
    - 장점:
        - 애플리케이션을 여러 클러스터(특히 지리적으로 분산된 클러스터)에 분산 배치하여 전체 클러스터 손실에 대비한 견고성을 확보할 수 있다.
        - 클러스터 업그레이드 시, 제자리 업그레이드보다 새로 업그레이드된 클러스터로 마이크로서비스를 옮기는 것이 더 쉽고 안전하다.
    - 단점:
        - 자원 풀링이 더 어려워진다.
        - 예를 들어, 한 클러스터에 유휴 자원이 많고 다른 클러스터가 완전히 활용될 때, 유휴 자원을 필요한 클러스터로 효율적으로 이동하기 어렵다.

### 8.5.4 클라우드 네이티브 컴퓨팅 재단

- **주요 역할**: 클라우드 네이티브 개발을 촉진하며, 쿠버네티스 관련 오픈소스 프로젝트의 생태계를 관리하고 발전시킨다.
- **운영 방식**: 프로젝트를 직접 개발하기보다, 독립적인 프로젝트들이 모여 공통 표준과 상호 운용성을 발전시킬 수 있는 협력의 장을 제공한다. (아파치 소프트웨어 재단과 유사)
- **핵심 기여**: 쿠버네티스의 성공에 결정적인 역할을 했으며, 클라우드 네이티브 기술 분야의 파편화를 방지하고 폭넓은 커뮤니티 지원을 이끌어내었다.

### 8.5.5 플랫폼과 이식성

- 쿠버네티스 사용자는 서비스 메시, 메시지 브로커, 로그 도구 등 지원 소프트웨어를 추가하여 결국 자신만의 '플랫폼'을 구축하게 된다.
    - 이는 CNCF 덕분에 호환성 높은 도구 생태계가 마련되어 가능하며, 특정 작업에 원하는 도구를 사용할 수 있다.
- 하지만 이러한 자유로운 선택은 오히려 과중한 선택 상황으로 이어져 혼란을 줄 수도 있다.
- **이식성의 한계**:
    - 쿠버네티스 위에서 구축된 애플리케이션은 이론적으로 클러스터 간 이식 가능하나, 실제로는 항상 그렇지 않다.

### 8.5.6 헬름, 오퍼레이터, CRD!

- 헬름(Helm):
    - 스스로를 쿠버네티스의 '누락된 패키지 매니저'라 칭한다.
    - 애플리케이션을 패키징하고 초기 설치를 관리하는 데 중점을 둔다.
- 오퍼레이터(Operator):
    - 애플리케이션의 초기 설치뿐만 아니라 지속적인 관리 및 운영에 더 중점을 둔다.
    - 쿠버네티스 위에서 애플리케이션의 동작을 자동화하여 관리자 대신 복잡한 작업을 수행한다.
- CRD (Custom Resource Definition):
    - 핵심 쿠버네티스 API를 확장하여 클러스터에 새로운 사용자 정의 리소스 유형과 동작을 추가할 수 있게 한다.
    - 기존 쿠버네티스 명령줄 인터페이스 및 접근 제어와 원활하게 통합되어, 쿠버네티스 추상화를 자체적으로 구현하는 것과 유사한 효과를 제공한다.
    - 구성 정보 관리, 서비스 메시 제어, 카프카와 같은 클러스터 기반 소프트웨어 관리에 활용될 수 있다.

### 8.5.7 Knative

- 쿠버네티스를 기반으로 개발자에게 FaaS(Function as a Service) 방식의 워크플로를 제공하는 오픈 소스 프로젝트
    - 쿠버네티스의 복잡성을 숨기고 FaaS의 개발자 경험을 쿠버네티스에 통합하는 것이 목표

### 8.5.8 미래

- 쿠버네티스의 인기는 지속될 것이며, 자체 클러스터 또는 관리형 클러스터 형태로 더 널리 사용될 것이다.
- 미래에는 Knative와 같이 쿠버네티스 위에 더 추상화된, 개발자 친화적인 플랫폼들이 등장하여 쿠버네티스는 숨겨진 기반 기술로 자리 잡을 것으로 예상한다.

### 8.5.9 사용해야 될까?

- 자체 클러스터 구축은 비추
- 관리형 클러스터: 공용 클라우드 사용이 가능하다면 Google, Azure, AWS 등에서 제공하는 완전 관리형 쿠버네티스 솔루션을 고려한다.
- 대안 고려: 공용 클라우드를 사용하는 경우, 쿠버네티스 대신 FaaS 플랫폼이나 Azure Web App, Google App Engine, Heroku와 같은 다른 PaaS 솔루션이 개발자 생산성 측면에서 더 적합할 수 있다.
- 결정 전 탐색: 쿠버네티스 사용을 결정하기 전에 minikube, MicroK8s 등을 로컬에서 실행해보거나 Katacoda, CNCF의 교육 자료를 통해 실제 사용자들이 충분히 경험하도록 한다.
- "남들이 다 하니까"라는 이유로 쿠버네티스를 선택하는 것은 위험하다.
    - 소수 개발자 팀이나 적은 수의 마이크로서비스를 운영하는 경우 관리형 플랫폼을 사용하더라도 쿠버네티스는 오버엔지니어링(Over-engineering)일 가능성이 높다.

## 8.6 점진적 제공

### 8.6.1 배포와 릴리스의 분리

> 배포는 소프트웨어의 일부 버전을 특정 환경(운영 환경)에 설치될 때 발생하는 것이다. 릴리스는 시스템이나 그 일부(예: 기능)를 사용자가 사용할 수 있도록 만드는 것이다.
> 
- 블루그린 배포
    - 하나의 소프트웨어 버전이 라이브(파란색)이 되면 운영 환경의 이전 버전(녹색)과 함께 새 버전을 배포한다.  (배포)
    - 새 버전이 예상대로 작동하는지 확인하고, 작동한다면 고객이 새 버전의 소프트웨어를 보도록 경로를 리디렉션 한다. (릴리스)

### 8.6.2 점진적 제공으로

- 점진적 제공은 지속적 제공(Continuous Delivery)의 확장이다. 이는 새로 출시되는 소프트웨어의 잠재적인 영향을 제어할 수 있는 기능을 제공하는 기술이다.
- 한 번에 모든 것을 배포하는 롤아웃이 아니라, 여러 단계를 거쳐 점진적으로 배포
- 선별된 사용자 그룹에 기능을 점진적으로 배포하여 성능과 안정성을 검증하고 위험을 줄이며 확장

### 8.6.3 기능 토글

- 기능 토글(기능 플래그)을 사용하면 기능을 끄거나 켜는 데 사용할 수 있는 토글 뒤에 배포된 기능을 숨길 수 있다.
    - 아직 완료되지 않은 기능을 체크인하고 배포할 수 있지만 최종 사용자에게는 숨긴다.
- 또는 요청하는 사용자의 특성에 따라 플래그가 다른 상태를 갖도록 설정할 수 있다.
- 기능 토글을 관리하는 완전 관리형 솔루션 → LaunchDarkly, Split

### 8.6.4 카나리아 릴리스

- 소프트웨어 배포 시 발생할 수 있는 실수로 인한 영향을 제한하는 기술
- 새로운 기능이나 버전의 소프트웨어를 제한된 소수의 고객에게 먼저 배포
- 작동 방식:
    - 문제 발생 시 소수 고객만 영향을 받는다.
    - 문제가 없으면 점진적으로 더 많은 고객에게 확대하여 배포한다.
    - 마이크로서비스 아키텍처에서는 마이크로서비스 수준에서 기능 토글(켜고 끄는 기능)을 구현하거나, 두 가지 버전의 마이크로서비스를 동시에 실행하며 라우팅을 통해 트래픽을 제어한다.
- 자동화: 과거에는 수동으로 트래픽 비율을 조절했으나, 요즘에는 스핀에이커(Spinnaker)와 같은 도구를 사용하여 에러율 등 메트릭을 기반으로 자동으로 트래픽을 늘리는 등 자동화하는 경우가 많다.

### 8.6.5 병렬 실행

- 카나리아 릴리스로는 확인하기 어려운, 새로운 기능이 이전 버전과 정확히 동일하게 작동하는지 비교 검증하는 방법
- 동일한 기능의 서로 다른 두 가지 구현체(보통 이전 버전과 새 버전)를 동시에 나란히 실행하고 동일한 요청을 두 구현체 모두에게 보낸 후 그 결과를 비교
- 작동 방식:
    - 동일한 요청을 서비스의 두 가지 버전으로 동시에 보낸다.
    - 두 구현체에서 나온 결과를 비교하여 기능의 일관성과 정확성을 검증한다.
    - 일반적으로 고객에게 실제 영향을 주지 않기 위해, 비교 시에는 하나의 구현체 결과(주로 기존 버전)만을 '신뢰할 수 있는 출처'로 사용하여 부작용을 방지한다.
