## 10.1 분열, 공황 그리고 혼란

- 모놀리식은 단일 장애 지점으로 원인 추적이 단순하지만, 마이크로서비스 장애는 아래와 같은 이유로 조사 자체가 어렵다
    - 서비스 수 증가 → 원인 후보 급증
    - 로그, 서버, 네트워크 지점 모두 조사 대상
- 필요한 접근
    - 작은 단위는 상세하게 모니터링
    - 큰 그림은 집계(aggregation)로 파악
    - 데이터를 쪼개서 분석할 수 있는 도구 필요
- 핵심 사고 전환
    - 운영 환경에서의 테스트 수용
    - “정상 상태(health)”에 대한 더 똑똑한 정의 필요

## 10.2 단일 마이크로서비스, 단일 서버

- 가장 단순한 운영 형태로, 하나의 서버와 하나의 마이크로서비스 인스턴스
- 모니터링 대상
    - 호스트 수준: CPU, 메모리 등 시스템 메트릭
    - 애플리케이션 로그: 오류, 예외, 사용자 문제
    - 외부 관찰: 응답 시간, 웹 서버 로그, 상태 확인(health check) 엔드포인트

## 10.3 단일 마이크로서비스, 다수 서버

- 여러 인스턴스 + 로드 밸런서 구조 → 모니터링 난이도 상승
- 요구 사항
    - 호스트별 메트릭 수집
    - 전체 집계 → 개별 분해 분석
- 응답 시간 측정은 로드 밸런서와 애플리케이션 양쪽에서 필요
- 로드 밸런서 역할은 비정상 인스턴스 제거하는 것으로 이 시점부터 “정상 상태란 무엇인가?”에 대한 정의가 중요해짐

## 10.4 다수 마이크로서비스, 다수 서버

- 문제 난이도 급증: 수천 줄의 로그와 다수 서비스 간 호출 체인
- 주요 과제
    - 개별 서버 문제 vs 시스템 전체 문제 구분
    - 호출 체인 깊숙한 오류 추적
- 단순 메트릭/로그 수집만으로는 부족
- 데이터 선별 능력, 관찰가능성 중심 사고, 운영 환경에서의 테스트 수용 필요

## 10.5 관찰가능성 대 모니터링

- 관찰가능성(Observability): 시스템의 속성
    - 외부 출력만으로 내부 상태를 이해할 수 있는 정도
- 모니터링: 우리가 수행하는 활동
- 전통적 모니터링 한계: “문제 될 것”을 미리 가정해야 하지만, 분산 시스템에서는 예상 불가능한 문제 다수 발생
- 관찰가능성이 높은 시스템의 장점: 이전에 생각 못 한 질문을 운영 시스템에 던질 수 있음 → 근본 원인 분석 속도 향상
- 모니터링 = 관찰가능성을 활용하는 행위

### 10.5.1 관찰가능성의 주축?

- 관찰가능성의 세 가지 주축: 메트릭, 로그, 분산 추적
- But,
    - 지나치게 **구현 중심적**
    - 개념 간 경계는 실제로 명확하지 않고, 메트릭 ↔ 로그 ↔ 추적은 서로 겹칠 수 있음
- 중요한 것은 **도구 수집이 아니라 시스템에 대한 이해이며,** 로그·메트릭·이벤트는 수단일 뿐
- 관점 전환
    - 시스템의 모든 외부 출력은 **이벤트**
    - 이벤트를 기반으로
        - 집계 → 메트릭
        - 검색 → 로그
        - 연관 → 추적

## 10.6 관찰가능성의 구성 요소

- 관찰가능성은 단일 도구가 아니라 여러 구성 요소의 조합
- 핵심 구성 요소
    - 로그 집계
    - 메트릭 집계
    - 분산 추적
    - “지금 괜찮은가?”를 판단하는 기준(SLA/SLO/SLI)
    - 알림
    - 시맨틱 모니터링
    - 운영 환경에서의 테스트

### 10.6.1 로그 집계

<img width="715" height="322" alt="Image" src="https://github.com/user-attachments/assets/1f1a3d22-51b4-4d37-a55b-b7fdedb189f9" />

- 분산 환경에서는 로그 집계 시스템 필요 → 각 서비스 로그를 중앙에서 수집·검색
- 로그의 역할: 장애 진단, 초기 이상 징후 감지, 운영 가시성의 핵심 수단
- 마이크로서비스에서 로그 집계는 선택이 아니라 필수

**📍공통 포맷**

- 로그 집계를 위해 **표준 로그 포맷 필수** (타임스탬프, 서비스 이름, 로그 레벨, 주요 식별자)
- 로그 전달 에이전트의 재포맷은 가급적 지양 → CPU 사용 증가 위험
- 가능하면 **서비스에서 직접 로그 포맷 통일**

**📍로그 문자열의 상관관계**

- 하나의 사용자 요청이 여러 서비스 호출로 확장하기 때문에 개별 서비스 로그만으로는 전체 흐름 파악 불가
- 해결책: 상관관계 ID(Correlation ID)
    - 최초 요청에서 생성
    - 모든 후속 호출에 전파
    - 로그에 동일한 위치로 기록
- 효과
    - 하나의 요청에 대한 전체 호출 흐름 추적 가능
    - 비용이 큰 트랜잭션, 이상 케이스 분석에 매우 유용
- 나중에 도입하기 매우 어렵기 때문에 로그 집계 도입 즉시 적용 권장

**📍타이밍**

- 머신 간 시계 오차 존재로 인해 로그 타임스탬프는 절대적 순서를 보장하지 않음
- 정확한 인과 관계 파악 어렵고 정확한 지연 시간 분석 불가
- 대안: 분산 추적 사용

**📍구현**

- 대표적 오픈 소스 스택: Fluentd + Elasticsearch + Kibana
    - 로그 전달 에이전트 Fluentd를 사용해 로그를 Elasticsearch로 전송하고 Kibana를 결과로 발생된 로그 스트림을 분할하고 분석
- 주의점
    - Elasticsearch는 검색 인덱스이지 DB가 아님
    - 로그 손실 시 재생성 가능 여부 고려 필요
    - 라이선스 변경(SSPL) 이슈 존재
- 대안
    - Kibana → Splunk (고가)
    - 로그 집계용 → Humio, Datadog
    - 클라우드 기본 도구(CloudWatch, App Insights 등)

**📍단점**

- 시간 왜곡 문제
- 데이터 폭증 → 저장 비용 증가, 인덱싱 비용 증가
- 확장성 문제 → 인덱스 유지 비용 큼
- 민감 정보 노출 위험 → 접근 제어 필요, 불필요한 데이터는 기록하지 않는 것이 최선

### 10.6.2 메트릭 집계

- 목적
    - 시스템 상태의 장기적 패턴 파악
    - 이상 탐지
    - 용량 계획
- 단일 값으로는 정상/비정상 판단 어려움 → 충분한 기간의 메트릭 축적 필요
- 요구 사항
    - 동적 인스턴스 환경 지원
    - 서비스/인스턴스 단위 집계
    - 메타데이터 연계
- 해상도 관리
    - 최근 데이터: 고해상도
    - 과거 데이터: 집계된 저해상도
- 한계
    - 무엇을 집계할지 사전에 결정해야 함
    - 집계 과정에서 정보 손실 발생

**📍낮은 카디널리티 vs 높은 카디널리티**

- 카디널리티: 쿼리 가능한 필드 조합 수
- 낮은 카디널리티 → CPU, 메모리, 요청 수 등
- 높은 카디널리티 → 고객 ID, 요청 ID, 주문 ID 등
- 전통적 메트릭 시스템은 높은 카디널리티에 취약
    - Prometheus: 낮은 카디널리티에 최적
- 높은 카디널리티는 사전에 예상하지 못한 질문 가능하고 관찰가능성 수준 비약적 향상

**📍구현**

- Prometheus
    - 표준적인 메트릭 수집 도구
    - 한계 명확 (높은 카디널리티)
- 대안: Honeycomb, Lightstep

### 10.6.3 분산 추적

- 목적
    - 서비스 간 호출 관계와 흐름 이해
    - 전체 요청 단위의 지연·병목 분석
- 로그 상관관계 ID는 출발점일 뿐
- 분산 추적은 시각화, 분석, 호출 간 관계 파악에 최적

**📍작동 원리**

<img width="896" height="597" alt="Image" src="https://github.com/user-attachments/assets/510d1791-368e-44be-a7d5-f7570d56c83b" />

- 스팬(span): 단일 작업 단위 (스레드 안의 로컬 활동)
- 트레이스(trace): 스팬들의 연결
- 스팬에 포함되는 정보: 시작/종료 시간, 로그, 키-값 메타데이터
- 샘플링 필수
    - 전체 수집은 시스템 부담 큼
    - 무작위 샘플링 → 기본
    - 동적 샘플링 → 오류 중심 수집 가능

**📍분산 추적 구현**

- 스팬 캡처, 수집기로 전송, 중앙 수집기 과정이 필요
- 표준 API
    - OpenTracing (과거)
    - OpenTelemetry (권장)
- 구현 방식
    - 라이브러리 자동 계측 + 커스텀 계측
    - 로컬 에이전트 사용 일반적
- 도구
    - 오픈 소스: Jaeger
    - 상용: Honeycomb, Lightstep

### 10.6.4 잘하고 있나요?

- 단순한 up/down 개념은 분산 시스템에 부적합
- 개별 서비스 상태 ≠ 전체 시스템 상태
- 관점 전환 필요: 개별 지표 → **사용자 관점의 동작**
- SRE(사이트 안정성 엔지니어링) 개념 도입: 안정성과 변화의 균형

**📍서비스 수준 계약 (SLA)**

- 사용자와의 계약
- 최소 기준에 가까움
- 달성해도 만족스럽지 않을 수 있음

**📍서비스 수준 목표 (SLO)**

- 팀 단위 목표
- SLA를 달성하기 위한 내부 기준
- 외부 요구 + 내부 개선 목표 포함 가능

**📍서비스 수준 지표 (SLI)**

- SLO 달성 여부를 측정하는 실제 지표
    - 예: 응답 시간, 오류율, 처리량

**📍오류 예산**

- 허용 가능한 실패량
- SLO에서 자동 계산 가능
- 효과
    - 안정성과 실험의 균형
    - 위험 감수 여부에 대한 객관적 기준 제공

### 10.6.5 알림

- 목적: 인간이 조치를 취해야 할 때만 알림
    - 하지만 시스템 복잡도 증가 → 알림 폭증

**📍어떤 문제는 더 심각하다**

- 모든 문제를 즉시 알릴 필요는 없음
    - “이 문제로 새벽 3시에 깨워야 하는가?”
- 장애 허용 설계의 중요성

**📍알림 피로**

- 알림 과다 → 판단력 저하
- 따라서 너무 많은 알림은 위험

**📍더 나은 알림을 위해**

- 좋은 알림의 조건(EEMUA)
    - 관련성: 경고할 가치가 있는지
    - 고유성: 경고가 다른 경고를 복제하지 않도록
    - 적시성: 알림을 활용할 수 있도록 신속하게 알림을 받아야 한다
    - 우선순위: 알림을 처리할 순서를 결정할 수 있도록 운영자에게 충분한 정보 제공
    - 명확성: 알림의 정보는 명확하고 읽기 쉽게
    - 진단 가능성: 무엇이 잘못됐는지 명확히
    - 자문: 운영자가 취해야 할 조치를 이해하도록 도와야 함
    - 집중: 가장 중요한 문제에 주목
- 알림 설계는 보내는 사람보다 받는 사람 관점이 중요

### 10.6.6 시맨틱 모니터링

- 시맨틱 모니터링(Semantic Monitoring): 시스템의 허용 가능한 의미론(semantic) 에 대한 모델을 정의 → “시스템이 기대한 대로 동작하는가?”
    - 신규 고객은 가입할 수 있다
    - 피크 시간대에 시간당 최소 2만 달러 매출을 발생시킨다
    - 주문한 상품은 정상 속도로 배송된다
- 목적
    - 오류 자체보다 시스템 상태의 의미를 이해
    - 오류 우선순위를 더 잘 판단할 수 있게 함
- 효과
    - 높은 수준의 시스템 건강도 판단
    - SLO 정의에 도움
- 어려움
    - 모델에 대한 합의
    - 기술팀 단독으로 결정 불가
    - PO 등 비즈니스 이해관계자 참여 필요
- 구현 방식
    - 실사용자 모니터링(RUM)
    - 합성 트랜잭션(운영 환경 테스트)

**📍실사용자 모니터링 (RUM)**

- 실제 운영 환경에서 발생한 사용자 행동을 관찰
    - 예: 가입자 수, 주문 수, 매출 금액
- 장점: 현실 반영도가 매우 높음
- 기존에 “비즈니스 지표”로만 보던 정보도 메트릭 저장소에 직접 기록하게 되며 알림 조건으로 활용 가능
- 한계
    - 고객이 실패를 경험한 후에야 문제를 알게 됨
    - 노이즈가 많아 해석이 어려움

### 10.6.7 운영 환경에서 테스팅

- 로컬/스테이징에서만 검증하는 것은 불충분하며 실제 시스템은 운영 환경에서만 드러나는 문제가 많음
- 핵심 관점
    - 운영 환경 테스트는 본질적으로 모니터링의 한 형태
    - 사용자가 알아채기 전에 문제를 발견·해결

**📍합성 트랜잭션 (Synthetic Transaction)**

- 가짜 사용자 행동을 운영 환경에 주입하여 입력과 예상 출력이 명확히 정의되도록 함
    - 예: 신규 고객 생성 → 성공 여부 확인
- 특징
    - 주기적으로 실행
    - 빠른 문제 감지 가능
- 장점
    - 노이즈가 적음
    - 사용자 영향 전에 문제 발견
- 단, 원인 분석에는 여전히 저수준 메트릭 필요

**📍합성 트랜잭션 구현 시 고려사항**

- 이미 엔드투엔드 테스트가 있다면 시맨틱 모니터링의 기반은 이미 존재하는 것
- 주의점
    - 테스트 데이터 관리
        - 운영 데이터 변화에 대응
        - 전용 테스트 데이터셋 사용 권장
    - 부작용 방지
        - 실제 주문, 실제 결제 발생 주의
        - 테스트 전용 사용자/경로 필수

**📍A/B 테스트**

- 동일 기능의 두 가지 버전을 사용자에게 분산 노출하여 어떤 버전이 더 효과적인지 검증
    - 예, 두 가지 가입 양식 중 전환율 비교

**📍카나리아 릴리스**

- 일부 사용자에게만 새 기능 노출하여, 성공 시 점진적으로 전체 사용자로 확대
- 장점: 문제 발생 시 영향 범위 최소화

**📍병렬 실행 (Parallel Run)**

- 동일 요청을 기존 구현과 신규 구현에 동시에 전달
- 사용자에게는 한 결과만 노출
- 장점
    - 두 구현의 결과·성능을 정확히 비교
    - 신규 구현의 부하 특성 이해에 유용

**📍스모크 테스트**

- 배포 직후 수행하는 기본 검증 테스트
- 범위
    - 프로세스 기동 여부 확인
    - 간단한 API 호출
    - 합성 트랜잭션 실행까지 다양
- 목적: “일단 살아 있는가?”

**📍카오스 엔지니어링**

- 운영 환경에 의도적으로 결함 주입
- 목적: 시스템이 예상된 실패를 견디는지 검증
- 효과
    - 인프라 장애에도 사용자 영향 최소화
    - 시스템 견고성 향상

## 10.7 표준화

- 마이크로서비스 아키텍처의 지속적 과제
    - 서비스별 자율성 ↔ 시스템 전체의 표준화
- 모니터링·관찰가능성은 표준화가 특히 중요한 영역
    - 여러 서비스가 협업하는 시스템을 하나의 관점에서 봐야 하기 때문

**📍표준화 대상**

- 로그: 표준 포맷 필수
- 메트릭: 모든 메트릭을 중앙 저장소에 집계하며 표준 메트릭 이름 필요
    - 예: `ResponseTime` ,`RspTimeSecs`
    - 같은 의미인데 이름이 다르면 분석·집계가 매우 어려워짐

**📍플랫폼 팀의 역할**

- 로그 집계, 메트릭 수집 같은 기본 관찰가능성 인프라를 플랫폼으로 제공
- 점점 더 많은 영역이 개별 팀이 아닌 플랫폼 팀 책임으로 이동

## 10.8 도구 선택

- 관찰가능성 도구 시장은 매우 빠르게 변화 중
- 현재의 도구가 미래에도 최선이라는 보장은 없음
- 따라서 특정 벤더보다 기준(criteria)을 먼저 세워라

### 10.8.1 민주적 선택

- 문제점
    - 숙련자만 쓸 수 있는 도구 → 운영 참여 인원 제한
    - 너무 비싼 도구 → 운영 환경에서만 사용 → 개발자는 늦게 문제 인지
- 원칙: 모든 팀원이 접근 가능해야 함
- 권장 사항
    - 개발·테스트·운영 환경에서 동일한 도구 사용
    - 공동 소유(shared ownership) 문화에 필수

### 10.8.2 쉬운 통합

- 분산 시스템에서는 다양한 형식의 정보 수집이 필수적이며 정보 추출과 도구 연동을 최대한 쉽게 가져가야 한다.
- 개방형 표준의 중요성: OpenTracing, OpenTelemetry
- 장점
    - 도구 간 통합 용이
    - 벤더 락인 최소화
    - 향후 도구 교체 비용 감소

### 10.8.3 맥락 제공

- 관찰가능성 도구의 핵심 가치 → “정보 + 맥락”
- 필요한 맥락 유형
    - 시간적 맥락: 과거와 비교해 어떻게 달라졌는가?
    - 상대적 맥락: 시스템 내 다른 요소와 비교하면?
    - 관계적 맥락: 의존 관계는 무엇인가?
    - 비례적 맥락: 얼마나 심각한가? 영향 범위는?

### 10.8.4 실시간성

- 관찰가능성 정보는 지금(now) 필요
- 기준: 분·시간 단위 ❌ 초 단위 ✔
- 목적
    - 사용자가 인지하기 전에 문제 감지
    - 불만 제기 시 즉시 상황 파악 가능

### 10.8.5 규모에 맞게

- 대규모 시스템 사례를 그대로 가져오는 것은 위험
    - 기능 희생, 공격적 샘플링
- 기준
    - 조직 규모에 맞는 기능
    - 비용 효율성
    - 성장 가능성까지 고려

## 10.9 기계화된 전문가

- 최근 트렌드 → AI / ML 기반 자동 이상 탐지
- 저자의 입장 → 전문 지식의 완전 자동화는 환상
- 도구는 “뭔가 이상하다”를 알려줄 수는 있지만 무엇을 해야 하는지 판단하는 것은 여전히 인간이 판단해야 한다.

## 10.10 시작하기

- 최소 출발점 체크리스트
    - 호스트 수준 메트릭
        - CPU, IO 등
        - 마이크로서비스 인스턴스와 매핑 가능해야 함
    - 서비스 메트릭
        - API 응답 시간
        - 다운스트림 호출 로그 기록
    - 로그
        - 상관관계 ID 포함하여 주요 비즈니스 단계 기록
    - 기본 인프라
        - 메트릭 + 로그 집계 파이프라인
- 분산 추적 도구
    - 자체 운영은 복잡성 증가
    - 관리형 서비스라면 초기 도입 고려 가능
- 합성 트랜잭션
    - 주요 연산에 대해 적극 고려하여 시스템 설계 단계부터 염두에 둘 것
- 궁극적 질문
    - “우리는 이 시스템이 사용자에게 제대로 동작하고 있다고 확신할 수 있는가?”